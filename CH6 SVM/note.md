## 第六章 SVM
SVM，即支持向量机(Support Vector Machines,SVM)，这个名字听起来很唬人啊，其实本质是一种二分类算法。我们都知道做二分类时会有很多种情况，SVM在所有可能的情况里选择间隔最大的。它最优秀的地方在于它可以和核技巧结合来成为非线性分类器。为了更好地理解SVM，我们从数学原理开始。

### 6.1原理
#### 6.1.1线性可分向量机
首先考虑最简单的情形，数据可分并且是线性可分的。也就是说在不做任何降维或者升维变换时，我们可以将两部分数据用一个超平面分开，并且平面方程有$wx=b$的形式。比如说二维数据，我们把所有数据点绘制到XY平面上，可以找到一条直线将它们分开。我们假设数据集为

$
T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}
$

其中$x_i\in{R^n},y_i\in{\{+1,-1\}}$，我们用$y_i$来表示$x_i$的分类，即正负两类。目的是找到一个超平面$wx=b$使得$x_i$被分到它的两侧，这时候我们导出分类规则：

记$f(x)=sign(wx+b)$为线性可分向量机。

接下来要寻找间隔最大的分类规则，首先定义间隔，我们引入函数间隔和几何间隔。

##### 函数间隔
对与给的的训练数据集T和超平面(w,b)，定义超平面关于样本点$(x_i,y_i)$的函数间隔为

$
\hat{\gamma_i}=y_i(w·x_i+b),
$

超平面关于训练集的函数间隔为其关于各样本点的函数间隔的最小值

 $
 \hat{\gamma}=\min\limits_{i=1,...N}\hat{\gamma_i}
 $

函数间隔能比较好的表示分类预测时的正确性和确信度，但是存在一个问题，如果我们等倍数的放大w和b，超平面不变，函数间隔却被放大了。因此我们需要给函数间隔加一些约束，比如规定$||w||=1$，这时我们得到了几何间隔

##### 几何间隔
为了能使得$||w||=1$，我们将超平面中x的系数归一化，规定超平面(w,b)关于样本点$(x_i,y_i)$的几何间隔为

$
\gamma_i=y_i(\frac{w}{||w||}·x_i+\frac{b}{||w||})
$

超平面关于训集的几何间隔为其关于数据点间隔的最小值

$
\gamma=\min\limits_{i=1,...,N}\gamma_i
$

因此几何间隔和函数间隔有下面的关系

$
\gamma_i=\frac{\hat{\gamma_i}}{||w||},\gamma=\frac{\hat{\gamma}}{||w||}
$

上面我们以及获得了间隔的定义，接下来寻找使间隔最大化的平面即可，用数学式表达即为一个约束最优化问题

$
\max\limits_{w,b} \gamma\\
s.t.  y_i(\frac{w}{||w||}·x_i+\frac{b}{||w||})\geq{\gamma}, i=1,2,...,N
$

利用几何间隔和函数间隔的关系可以将问题改写为

$
\max\limits_{w,b} \frac{\hat{\gamma}}{||w||}\\
s.t. y_i(w·x_i+b)\geq{\hat{\gamma}}, i=1,2,...,N
$

观察发现，如果让w和b按比例变成$\lambda{w}$和$\lambda{b}$，函数间隔变成$\lambda\gamma$，对约束和最大化目标均没有影响。因此，为了简化问题，我们令$\hat{\gamma}$






