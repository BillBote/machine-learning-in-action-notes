# 第八章 预测数值型数据：回归
这一章主要介绍线性回归，包括最基本的线性回归、局部加权线性回归、岭回归和逐步线性回归以及一个预测鲍鱼年龄和玩具售价的案例。

## 1.用线性回归寻找最佳拟合直线
回归的目的是获得预测值，最简单的想法就是得到一个目标值的计算公式。比如预测雨量时，可能会这样计算：
rainQuality=0.0015\*date-0.02\*windSpeed
这就是所谓的回归方程(regression equation)，其中0.0015和-0.02称为回归系数。当回归方程是线性方程时，称为线性回归(linear regression)。
### 回归的一般方法

假设输入数据(也可以称为自变量)储存在矩阵X中，回归系数储存在w中，我们希望获得方程$Y=X^Tw$得到准确的预测值，问题在于我们如何利用手上的数据获得w以及使误差最小。考虑到误差有正有负，我们使用平方误差。很自然的想法就是让平方误差最小，即最小化：$\sum_{i=1}^{n}(y_i-x_{i}^{T}w)^2$
用矩阵写作$(y-Xw)^T(y-Xw)$。如果对w求导并使其为0，解出w如下：$$\hat{w}=(X^TX)^{-1}X^Ty$$
注意这里有对$X^TX$的求逆运算，因此在代码里要确认其是否可逆。这种方法被称为最小二乘法。
只要$X^TX$可逆，就能用这个方法建立关于任何数据的回归模型，这个模型有如下性质：

(a)$E(\hat{\beta})=\beta$；

(b)$Cov(\hat{\beta})=\sigma^2(X^TX)^{-1}$；

(c)$c'\beta$的所有线性无偏估计里，$c'\hat{\beta}$是唯一具有最小方差的估计；

(d)$RSS=y'(I-X(X'X)^{-1}X')y$;

(e)$\hat{\sigma^2}=\frac{RSS}{n-p}$是$\sigma^2$的无偏估计

如果进一步假设误差向量$e\sim N(0,\sigma^2I)$,则有以下性质：

(a)$\hat{\beta} \sim N(\beta,\sigma^2(X'X)^{-1})$；

(b)$\frac{RSS}{\sigma^2} \sim \chi^2_{n-p}$；

(c)$\hat{\beta}$与RSS相互独立。

对于$\hat{\beta}$的每一个分量也由类似性质：

(a)$\hat{\beta_i}\sim N(\beta_i,\sigma^2\[(X'X)^{-1}\]\_{i+1,i+1})$;

(b)在$\beta_i$的一切线性无偏估计中，$\hat{\beta_i}$是唯一方差最小的估计。

regression.py里的standRegres函数实现了最基础的最小二乘。

回归中有个很显然的问题，如果不同的数据得到了相同的回归直线，如何比较优劣程度，为此我们引入一个量

$
R^2=\frac{SS_{回}}{SS_{总}}
$

其中$SS_{回}=\hat{\beta'X'_cy}=y'X_c(X'_cX_c)^{-1}X'_cy$称为回归平方和
