# 第八章 预测数值型数据：回归
这一章主要介绍线性回归，包括最基本的线性回归、局部加权线性回归、岭回归和逐步线性回归以及一个预测鲍鱼年龄和玩具售价的案例。

## 8.1 用线性回归寻找最佳拟合直线
回归的目的是获得预测值，最简单的想法就是得到一个目标值的计算公式。比如预测雨量时，可能会这样计算：
rainQuality=0.0015\*date-0.02\*windSpeed
这就是所谓的回归方程(regression equation)，其中0.0015和-0.02称为回归系数。当回归方程是线性方程时，称为线性回归(linear regression)。
### 回归的一般方法

假设输入数据(也可以称为自变量)储存在矩阵X中，回归系数储存在w中，我们希望获得方程$Y=X^Tw$得到准确的预测值，问题在于我们如何利用手上的数据获得w以及使误差最小。考虑到误差有正有负，我们使用平方误差。很自然的想法就是让平方误差最小，即最小化：$\sum_{i=1}^{n}(y_i-x_{i}^{T}w)^2$
用矩阵写作$(y-Xw)^T(y-Xw)$。如果对w求导并使其为0，解出w如下：$$\hat{w}=(X^TX)^{-1}X^Ty$$
注意这里有对$X^TX$的求逆运算，因此在代码里要确认其是否可逆。这种方法被称为最小二乘法。
只要$X^TX$可逆，就能用这个方法建立关于任何数据的回归模型，这个模型有如下性质：

(a)$E(\hat{\beta})=\beta$；

(b)$Cov(\hat{\beta})=\sigma^2(X^TX)^{-1}$；

(c)$c'\beta$的所有线性无偏估计里，$c'\hat{\beta}$是唯一具有最小方差的估计；

(d)$RSS=y'(I-X(X'X)^{-1}X')y$;

(e)$\hat{\sigma^2}=\frac{RSS}{n-p}$是$\sigma^2$的无偏估计

如果进一步假设误差向量$e\sim N(0,\sigma^2I)$,则有以下性质：

(a)$\hat{\beta} \sim N(\beta,\sigma^2(X'X)^{-1})$；

(b)$\frac{RSS}{\sigma^2} \sim \chi^2_{n-p}$；

(c)$\hat{\beta}$与RSS相互独立。

对于$\hat{\beta}$的每一个分量也由类似性质：

(a)$\hat{\beta_i}\sim N(\beta_i,\sigma^2\[(X'X)^{-1}\]\_{i+1,i+1})$;

(b)在$\beta_i$的一切线性无偏估计中，$\hat{\beta_i}$是唯一方差最小的估计。

regression.py里的standRegres函数实现了最基础的最小二乘。

回归中有个很显然的问题，如果不同的数据得到了相同的回归直线，如何比较优劣程度，为此我们引入一个量

$
R^2=\frac{SS_{回}}{SS_{总}}
$

其中$SS_{回}=\hat{\beta}'X'_cy=y'X_c(X'_cX_c)^{-1}X'_cy$称为回归平方和,

$
SS_{总}=(y-\hat{\alpha}1)'(y-\hat{\alpha}1)=(y-\bar{y}1)'(y-\bar{y}1)
$

称为修正的总平方和，$R^2$称为判定系数或测定系数。它度量了回归自变量对因变量的拟合程度，$0\geq{R^2}\geq1$，值越大相依性越强。

### 约束最小二乘
上述问题中，$\beta$没有约束，我们现在引入约束$A\beta=b$，这是个相容性方程组，其中$A$为$k\times{p}$的已知矩阵，且秩为$k$，我们利用拉格朗日乘子法求解。我们构造辅助函数$F(\beta)=(y-X\beta)'(y-X\beta)+2\lambda(A\beta-b)$，并对$\beta$求偏导并让其等于0

$
-X'y+X'X\beta+A'\lambda=0
$

解它与$A\beta=b$的联立方程组得到：

$
\left\{
\begin{aligned}
\hat{\lambda_c}=(A(X'X)^{-1}A')^{-1}(A\hat{\beta}-b)\\
\hat{\beta_c}=\hat{\beta}-(X'X)^{-1}A'(A(X'X)^{-1}A')^{-1}(A\hat{\beta}-b).\\
$

其中$\hat{\beta}$为无约束下的估计。

### 回归诊断

在上面的讨论中，我们做了一些假设，其中最重要的是Gauss-Markov假设，在某些场合还做了误差的正态性假设，因此我们面对真实数据时需要检验这些假设，这就是回归诊断。

#### 残差分析
