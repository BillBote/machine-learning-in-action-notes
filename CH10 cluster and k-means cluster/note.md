# 第十章 聚类和k均值聚类
对数据分类是一种常见分析方法，如果我们已知数据的类别并且已经给数据分好了类，那么我们可以利用分类方法（classification）进行分类。但是某些情况下，只有数据没有标签，我们怎么办。一个想法就是把相似的数据放到一起，这样就自然形成了几类。那么问题来了，什么样的数据是相似，也就是以什么样的标准定义相似度。直观的想法就是离的近的放一类呗，这就是用距离刻画相似度；或者相关系数大的方一起呗，这就是用相关系数刻画相似度。我们首先介绍这些度量。
## 10.1 标准
### 10.1.1 距离标准
z拿到数据第一反应一般是可视化，把数据处理为图像后，感觉聚类变得非常清晰——离得近的聚成一类。这就涉及到数据之间的距离。泛函分析里我们已经引入了距离公理，但聚类分析里的距离却不要求满足这些，我们记数据点$x=(x_1,x_2,...,x_n)$，以下是一些常用距离：

(i)欧氏距离，即L2范数：

$
d_{ij}^{(2)}=(\sum_{t=1}^n(x_i-x_j)^2)^{1/2}
$

(ii)Minkowski距离：

$
d_{ij}^{(q)}=(\sum_{t=1}^n|x_i-x_j|^q)^{1/q}
$

(iii)Chebyshev距离：

$
d_{ij}^{(c)}=\max\limits_{1\geq{t}\geq{n}}|x_{it}-x_{jt}|
$

(iv)马氏距离：

$
d_{ij}^{(m)}=(x_i-x_j)'V^{-1}(x_i-x_j)
$

其中，$V=\frac{1}{n}X'HX$，这里$H=I_n-\frac{1}{n}11'$，且V可逆

(v)兰氏距离：

$
d_{ij}^{(l)}=\sum_{t=1}^n\frac{|x_{it}-x_{jt}|}{x_{it}+x_{jt}}
$

一般在${x_{it},x_{jt},t=1,..,n}$都同号时使用。

上述各距离越小意味着数据点越接近，如果距离为0说明在某种意义下，两者是相同的。

### 10.1.2 相似系数
我们在多元统计中曾经引入过相关系数，即

$
r_{ij}^{(1)}=\frac{\sum_{t=1}^n(x_{ti}-\bar{x_i})(x_{tj}-\bar{x_j})}{(\sum_{t=1}^n(x_{ti}-\bar{x_i})^2\sum_{t=1}^n(x_{tj}-\bar{x_j})^2)^{1/2}}
$

此外，如果我们把数据点视为$R_n$空间上的向量，考虑两个向量之间的夹角，显然当夹角越小时两向量越接近，因此可以引出相似系数：

$
r_{ij}^{(2)}=\frac{\sum_{t=1}^nx_{ti}x_{tj}}{}
$

如果我们记以下表示：

$
\widetilde{x_{(i)}}=(x_{1i}-\bar{x_i},...,x_{ni}-\bar{x_i})'
$

那么我们发现$r_{ij}^{(1)}$实际上也是相似系数。

出于简单考虑，我们会使用同号率作为相关和相似系数的替代

即
$
n^+=x_{(i)}和x_{(j)}同号元素个数，n^-=x_{(i)}和x_{(j)}异号元素个数
$

那么$r_{ij}=\frac{n^+-n^-}{n^++n^-}$。对$\widetilde{x_{(i)}}$使用得到非参数相关系数，对$x_{(i)}$使用得到非参数相似系数。

除此之外，还有以下两种表示：

$
r_{ij}=\sum_{t=1}^n\min\limits_{i,j}(x_{ti},x_{tj})/\sum_{t=1}^n\max\limits_{i,j}(x_{ti},x_{tj})

r_{ij}=2\sum_{t=1}^n\min\limits_{i,j}(x_{ti},x_{tj})/\sum_{t=1}^n(x_{ti}+x_{tj})
$

上述式子均可用来计算相似度。

## 10.2 系统聚类
上一节介绍完聚类标准后，我们开始真正进入聚类。那么怎么开始呢？我们手上有数据点，作图？很直观但没法从图中直接得到类别，那么我们就先把各点看作不同的类，每次缩小类的数量不就ok了？这就是系统聚类的思路。把各点视为不同的类，进行迭代计算，每次迭代计算各类距离或相似系数，把最大的两类聚在一起，使总类数下降。这样我们会得到一个聚类图，由于类数不断减少，我们计算出的聚类标准不断减小，因此只要我们给定一个最小标准值，就能在聚类图中找到对应的聚类情况。

我们列出具体步骤

1.将每一个数据点视为一个类，计算类之间的归类指标（相似系数取绝对值），将指标值最大的归为一类；

2.确定新类的归类指标和归类方法，这有好几种方法，我们之后再说，计算得到新类的指标值；

3.逐次归类，即重复第2步操作，直到所有数据归为一类为止；

4.绘制聚类图，根据实际需要选择归类数目或指标值的阈值，在图中选出分类情况。

关于新类的归类指标计算，有综合变量法、平均系数法、最大系数法、最小系数法等。

(1)综合变量法：把新类中的变量进行加权求和作为新类别的变量；

(2)平均系数法：对分到新类中的变量关于其他类别变量的指标进行加权求和得到新变量和其他变量之间的指标值；

(3)最大系数法：取新变量中各类关于其他类别的指标值的最大值作为新变量的指标值；

(4)最小系数法：取新变量中各类关于其他类别的指标值的最小值作为新变量的指标值；

### 10.3 一次成形聚类
上一节讲到了系统聚类，它有一个很显著的问题就是计算量巨大，当数据量很大时计算量极其大，因此需要一种方法能减少计算量，这就是我们将要介绍的一次成形聚类。方法同样是把数据点各看作一类，计算类之间的指标，但我们只需要计算一次，把他们写成矩阵的形式。首先选出除对角线外最大的值，将它对应的两个点为一类，然后选出次大的同样分类，依次类推。

具体步骤：

1.计算个点之间的指标，列出矩阵，其中元素$a_{ij}$表示第i个和第j个点之间的指标；

2.选择除对角线外最大的值$a_{ij}$，将i、j分为一类；

3.选择次大的值，将这两点分为一类，有如下几种情况：

两点均未分类，则两点并为一类；

两点其中一点已被分类，另一点未分，则将未分类的一点分入已知类；

两点都被分类且不同类，则把两类合并；

4.重复步骤3，直到全部分完。

### 10.4 k水准逐步形成聚类

这次我们设定一个临界常数k，以此为准则逐步聚类。

第一步，计算n个变量之间的指标值，列成矩阵的形式，记作$R^{(0)}$确定临界值$0\geq{k}\geq{1}$；

第二步，从$R^{(0)}$除对角线之外的元素中取最大的$r_{ij}$，当$r_{ij}<k$，则将n个变量分成一类，结束聚类；否则将ij分为一类，记作$G^{(1)$，这时取p为其中的代表变量，然后按照$r_{pi}$的大小决定它是否分入此类：当$r_{pi}\geq{k}$时，纳入此类；否则不纳入。以此类推直到此类中不能再加入变量，此时记$G^{(1)}_1$中有$n_1^{(1)}$个变量，为$j_{11}^{(1)},…,j_{1n_{1}^{(1)}}^{(1)}$；

第三步，从$R^{(0)}$中去除已归类变量的行和列，从剩下的元素中找出最大值$r_{p’q’}$：当$r_{p’q’}<k$时，第一轮聚类结束，此时$G^{(1)}_1$为一类，剩下的变量各为一类；当$r_{p’q’}\geq{k}$时，按第二步算法聚类，得到$G^{(1)}_2$类，其中有$n_2^{(1)}$个变量，为$j_{21}^{(1)},…,j_{2n_{2}^{(1)}}^{(1)}$；

第四步，重复第三步，得到第一轮聚类结果：$G^{(1)}_1,…,G^{(1)}_{l_1}$;

第五步，按均值公式

$
r_{G^{(1)}_iG^{(1)}_j}=\frac{1}{n_i^{(1)}n_j^{(1)}}\sum_{u=1}^{}n_i^{(1)}\sum_{v=1}^{n_j^{(1)}}r_{j^{(1)}_{iu}j^{(1)}_{jv}}, i,j=1,…,l_1
$

计算出$G^{(1)}_1,…,G^{(1)}_{l_1}$的指标阵：如果指标阵中最大元素小于k，则聚类结束，结果为第一轮聚类结果；指标阵中最大元素大于k，如果则以$G^{(1)}_i$为变量，重复第二、三、四步，得到第二轮聚类结果

$
$G^{(2)}_1,…,G^{(2)}_{l_2}$.
$

接着进行第三轮；

第六步，假设第m轮聚类时计算的指标阵最大元素小于k，则聚类结束，此时最终结果为第m-1轮聚类结果。




