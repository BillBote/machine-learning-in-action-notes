# 第十章 聚类和k均值聚类
对数据分类是一种常见分析方法，如果我们已知数据的类别并且已经给数据分好了类，那么我们可以利用分类方法（classification）进行分类。但是某些情况下，只有数据没有标签，我们怎么办。一个想法就是把相似的数据放到一起，这样就自然形成了几类。那么问题来了，什么样的数据是相似，也就是以什么样的标准定义相似度。直观的想法就是离的近的放一类呗，这就是用距离刻画相似度；或者相关系数大的方一起呗，这就是用相关系数刻画相似度。我们首先介绍这些度量。
## 10.1 标准
### 10.1.1 距离标准
z拿到数据第一反应一般是可视化，把数据处理为图像后，感觉聚类变得非常清晰——离得近的聚成一类。这就涉及到数据之间的距离。泛函分析里我们已经引入了距离公理，但聚类分析里的距离却不要求满足这些，我们记数据点$x=(x_1,x_2,...,x_n)$，以下是一些常用距离：

(i)欧氏距离，即L2范数：

$
d_{ij}^{(2)}=(\sum_{t=1}^n(x_i-x_j)^2)^{1/2}
$

(ii)Minkowski距离：

$
d_{ij}^{(q)}=(\sum_{t=1}^n|x_i-x_j|^q)^{1/q}
$

(iii)Chebyshev距离：

$
d_{ij}^{(c)}=\max\limits_{1\geq{t}\geq{n}}|x_{it}-x_{jt}|
$

(iv)马氏距离：

$
d_{ij}^{(m)}=(x_i-x_j)'V^{-1}(x_i-x_j)
$

其中，$V=\frac{1}{n}X'HX$，这里$H=I_n-\frac{1}{n}11'$，且V可逆

(v)兰氏距离：

$
d_{ij}^{(l)}=\sum_{t=1}^n\frac{|x_{it}-x_{jt}|}{x_{it}+x_{jt}}
$

一般在${x_{it},x_{jt},t=1,..,n}$都同号时使用。

上述各距离越小意味着数据点越接近，如果距离为0说明在某种意义下，两者是相同的。

### 10.1.2 相似系数
我们在多元统计中曾经引入过相关系数，即

$
r_{ij}^{(1)}=\frac{\sum_{t=1}^n(x_{ti}-\bar{x_i})(x_{tj}-\bar{x_j})}{(\sum_{t=1}^n(x_{ti}-\bar{x_i})^2\sum_{t=1}^n(x_{tj}-\bar{x_j})^2)^{1/2}}
$

此外，如果我们把数据点视为$R_n$空间上的向量，考虑两个向量之间的夹角，显然当夹角越小时两向量越接近，因此可以引出相似系数：

$
r_{ij}^{(2)}=\frac{\sum_{t=1}^nx_{ti}x_{tj}}{}
$

如果我们记以下表示：

$
\widetilde{x_{(i)}}=(x_{1i}-\bar{x_i},...,x_{ni}-\bar{x_i})'
$

那么我们发现$r_{ij}^{(1)}$实际上也是相似系数。

出于简单考虑，我们会使用同号率作为相关和相似系数的替代

即
$
n^+=x_{(i)}和x_{(j)}同号元素个数，n^-=x_{(i)}和x_{(j)}异号元素个数
$

那么$r_{ij}=\frac{n^+-n^-}{n^++n^-}$。对$\widetilde{x_{(i)}}$使用得到非参数相关系数，对$x_{(i)}$使用得到非参数相似系数。

除此之外，还有以下两种表示：

$
r_{ij}=\sum_{t=1}^n\min\limits_{i,j}(x_{ti},x_{tj})/\sum_{t=1}^n\max\limits_{i,j}(x_{ti},x_{tj})

r_{ij}=2\sum_{t=1}^n\min\limits_{i,j}(x_{ti},x_{tj})/\sum_{t=1}^n(x_{ti}+x_{tj})
$

上述式子均可用来计算相似度。

## 10.2 系统聚类
上一节介绍完聚类标准后，我们开始真正进入聚类。那么怎么开始呢？我们手上有数据点，作图？很直观但没法从图中直接得到类别，那么我们就先把各点看作不同的类，每次缩小类的数量不就ok了？这就是系统聚类的思路。把各点视为不同的类，进行迭代计算，每次迭代计算各类距离或相似系数，把最大的两类聚在一起，使总类数下降。这样我们会得到一个聚类图，由于类数不断减少，我们计算出的聚类标准不断减小，因此只要我们给定一个最小标准值，就能在聚类图中找到对应的聚类情况。

我们列出具体步骤

1.将每一个数据点视为一个类，计算类之间的归类指标（相似系数取绝对值），将指标值最大的归为一类；

2.确定新类的归类指标和归类方法，这有好几种方法，我们之后再说，计算得到新类的指标值；

3.逐次归类，即重复第2步操作，直到所有数据归为一类为止；

4.绘制聚类图，根据实际需要选择归类数目或指标值的阈值，在图中选出分类情况。

关于新类的归类指标计算，有综合变量法、平均系数法、最大系数法、最小系数法等。

(1)综合变量法：把新类中的变量进行加权求和作为新类别的变量；

(2)平均系数法：对分到新类中的变量关于其他类别变量的指标进行加权求和得到新变量和其他变量之间的指标值；

(3)最大系数法：取新变量中各类关于其他类别的指标值的最大值作为新变量的指标值；

(4)最小系数法：取新变量中各类关于其他类别的指标值的最小值作为新变量的指标值；

### 10.3 一次成形聚类
上一节讲到了系统聚类，它有一个很显著的问题就是计算量巨大，当数据量很大时计算量极其大，因此需要一种方法能减少计算量，这就是我们将要介绍的一次成形聚类。方法同样是把数据点各看作一类，计算类之间的指标，但我们只需要计算一次，把他们写成矩阵的形式。首先选出除对角线外最大的值，将它对应的两个点为一类，然后选出次大的同样分类，依次类推。

具体步骤：

1.计算个点之间的指标，列出矩阵，其中元素$a_{ij}$表示第i个和第j个点之间的指标；

2.选择除对角线外最大的值$a_{ij}$，将i、j分为一类；

3.选择次大的值，将这两点分为一类，有如下几种情况：

两点均未分类，则两点并为一类；

两点其中一点已被分类，另一点未分，则将未分类的一点分入已知类；

两点都被分类且不同类，则把两类合并；

4.重复步骤3，直到全部分完。

### 10.4 k水准逐步形成聚类

这次我们设定一个临界常数k，以此为准则逐步聚类。

第一步，计算n个变量之间的指标值，列成矩阵的形式，记作$R^{(0)}$确定临界值$0\geq{k}\geq{1}$；

第二步，从$R^{(0)}$除对角线之外的元素中取最大的$r_{ij}$，当$r_{ij}<k$，则将n个变量分成一类，结束聚类；否则将ij分为一类，记作$G_1^{(1)}$，这时取p为其中的代表变量，然后按照$r_{pi}$的大小决定它是否分入此类：当$r_{pi}\geq{k}$时，纳入此类；否则不纳入。以此类推直到此类中不能再加入变量，此时记$G_1^{(1)}$中有$n_1^{(1)}$个变量，为$j_{11}^{(1)},…,j_{1n_{1}^{(1)}}^{(1)}$；

第三步，从$R^{(0)}$中去除已归类变量的行和列，从剩下的元素中找出最大值$r_{p’q’}$：当$r_{p’q’}<k$时，第一轮聚类结束，此时$G_1^{(1)}$为一类，剩下的变量各为一类；当$r_{p’q’}\geq{k}$时，按第二步算法聚类，得到$G_2^{(1)}$类，其中有$n_2^{(1)}$个变量，为$j_{21}^{(1)}…,j_{2n_{2}^{(1)}}^{(1)}$；

第四步，重复第三步，得到第一轮聚类结果：$G_1^{(1)},…,G_{l_1}^{(1)}$;

第五步，按均值公式

$
r_{G_i^{(1)}G_j^{(1)}}=\frac{1}{n_i^{(1)}n_j^{(1)}}\sum_{u=1}^{}n_i^{(1)}\sum_{v=1}^{n_j^{(1)}}r_{j_{iu}^{(1)}j_{jv}^{(1)}}, i,j=1,…,l_1
$

计算出$G_1^{(1)},…,G_{l_1}^{(1)}$的指标阵：如果指标阵中最大元素小于k，则聚类结束，结果为第一轮聚类结果；指标阵中最大元素大于k，如果则以$G_i^{(1)}$为变量，重复第二、三、四步，得到第二轮聚类结果

$
G_1^{(2)},…,G_{l_2}^{(2)}.
$

接着进行第三轮；

第六步，假设第m轮聚类时计算的指标阵最大元素小于k，则聚类结束，此时最终结果为第m-1轮聚类结果。

### 10.5 移动中心聚类（kMeans）
#### 10.5.1 k均值聚类算法
这种方法运用很广泛，并且其数学性不强，不需要非常高级的数学知识，但在实际运用中很有效，这也是机器学习令人着迷的地方，没有数学或统计原理支持却能取得非常优秀的表现，着实令人难忘。

在描述算法前先约定一些标记：一共n个样本，每个样本是p维的，用I表示样本集合，用欧氏距离表现样本间的差异。

第一步，选定k个中心，可以随机选择，记为$C_1^1,…,C_k^1$，接着把样本分到k类里

$
I_1^1,…,I_k^1
$

分类准则是计算样本到k个中心的距离，将它分到距离最近的类别里，这步划分记为$P_1$;

第二步，根据上一步分好的类，计算每一类的重心作为下一步分类的中心$C_1^2,…,C_k^2$。假设类$I_i^1$的每个样本有一个权重$p_j$使得$\sum_jp_j=1$，则

$
C_i^2=\frac{1}{\sum_jp_j}\sum_jp_jx_j, i=1,…,k
$

对这些新中心，重复第一步，重新划分为k类，这次划分记为$P_2$;

依此类推。

遇到以下情况时停止：

(1)相继的两次迭代结果相同；

(2)一个事先选定的准则值（如类内离差平方和）不再减少；

(3)达到事先规定的最大迭代步数。

我们可以证明每次迭代都能使类内离差平方和减小，这保证了算法的合理性。

算法实现即kMeans.py里的kMeans函数，算法很简单初始的中心随机选择以及分类和中心的迭代更新。在函数里我们没有规定迭代次数，仅仅利用判定条件的(1)，当然也可以加上迭代次数避免次数过多。

#### 10.5.2 聚类性能和二分k均值聚类算法
看起来聚类算法是很简单的，我们似乎已经解决了这个问题。但我们回想以下聚类过程，分类个数k是我们事先给定的，在不知道数据真实分类情况的时候，我们随机提出的分类数是否正确呢，这就需要我们继续讨论，实际上k均值聚类并没有收敛到全局最小值，也就是说k均值聚类最后收敛到的分类结果不能保证效果最优。我们可以用误差平方和（SSE）来度量聚类效果，简单的说就是各个数据点到中心的距离平方和。如何降低SSE就成为改进该方法的关键，如果我们增大k值，SSE必然减小，但这与聚类目标相悖。那么我们拆掉SSE最大的类，再合并两个类不就可以保证类的个数不增了吗。这确实是一个很有效的方法，只要让合并类导致的SSE增大小于拆分类导致的SSE减少即可，我们不断重复这种操作，就能在原有分类的基础上不断优化，直到得到一个让SSE最小的聚类结果，这就是满足全局最优的结果。

根据这个思想，我们发现只要能给出任一种聚类结果就可以优化下去，那么有人提出了基于这种思想的二分k均值聚类算法。我们首先将所有点归为一类，然后沿着使SSE下降最多的方向将其一分为二；接着选择一个类再次切分，使得SSE下降最快，逐次进行直到达到指定的类数（很像最速下降法对不对）。



