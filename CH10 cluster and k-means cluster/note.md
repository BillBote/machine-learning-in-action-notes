# 第十章 聚类和k均值聚类
对数据分类是一种常见分析方法，如果我们已知数据的类别并且已经给数据分好了类，那么我们可以利用分类方法（classification）进行分类。但是某些情况下，只有数据没有标签，我们怎么办。一个想法就是把相似的数据放到一起，这样就自然形成了几类。那么问题来了，什么样的数据是相似，也就是以什么样的标准定义相似度。直观的想法就是离的近的放一类呗，这就是用距离刻画相似度；或者相关系数大的方一起呗，这就是用相关系数刻画相似度。我们首先介绍这些度量。
## 10.1 标准
### 10.1.1 距离标准
z拿到数据第一反应一般是可视化，把数据处理为图像后，感觉聚类变得非常清晰——离得近的聚成一类。这就涉及到数据之间的距离。泛函分析里我们已经引入了距离公理，但聚类分析里的距离却不要求满足这些，我们记数据点$x=(x_1,x_2,...,x_n)$，以下是一些常用距离：

(i)欧氏距离，即L2范数：

$
d_{ij}^{(2)}=(\sum_{t=1}^n(x_i-x_j)^2)^{1/2}
$

(ii)Minkowski距离：

$
d_{ij}^{(q)}=(\sum_{t=1}^n|x_i-x_j|^q)^{1/q}
$

(iii)Chebyshev距离：

$
d_{ij}^{(c)}=\max\limits_{1\geq{t}\geq{n}}|x_{it}-x_{jt}|
$

(iv)马氏距离：

$
d_{ij}^{(m)}=(x_i-x_j)'V^{-1}(x_i-x_j)
$

其中，$V=\frac{1}{n}X'HX$，这里$H=I_n-\frac{1}{n}11'$，且V可逆

(v)兰氏距离：

$
d_{ij}^{(l)}=\sum_{t=1}^n\frac{|x_{it}-x_{jt}|}{x_{it}+x_{jt}}
$

一般在${x_{it},x_{jt},t=1,..,n}$都同号时使用。

上述各距离越小意味着数据点越接近，如果距离为0说明在某种意义下，两者是相同的。

### 8.1.2 相似系数
我们在多元统计中曾经引入过相关系数，即

$
r_{ij}^{(1)}=\frac{\sum_{t=1}^n(x_{ti}-\bar{x_i})(x_{tj}-\bar{x_j})}{(\sum_{t=1}^n(x_{ti}-\bar{x_i})^2\sum_{t=1}^n(x_{tj}-\bar{x_j})^2)^{1/2}}
$

此外
